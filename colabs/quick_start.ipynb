{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTrunCDp6HAD"
      },
      "outputs": [],
      "source": [
        "# @title LICENSE\n",
        "# Licensed under the Apache License, Version 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHA2XM4sTMfz"
      },
      "source": [
        "## JaxPruner Quick Start\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-research/jaxpruner/blob/main/colabs/quick_start.ipynb)\n",
        "\n",
        "This interactive colab provides a short overview of some of the key features of the `jaxpruner` library:\n",
        "\n",
        "- One-shot Pruning\n",
        "- Pruning during Optimization (Integration w/ optax)\n",
        "- ConfigDict Integration\n",
        "- Compatibility with JAX parallelization via `pmap` and `pjit`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdzHCJuop4ZB"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import flax\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.sharding import PartitionSpec\n",
        "import jax.experimental.pjit\n",
        "import numpy as np\n",
        "import optax\n",
        "import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2cBBZEk1_DG"
      },
      "outputs": [],
      "source": [
        "!pip3 install git+https://github.com/google-research/jaxpruner\n",
        "import jaxpruner\n",
        "import ml_collections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaU4vsg-tfrq"
      },
      "source": [
        "\n",
        "# One-shot Pruning\n",
        "Pruning a given matrix to a desired level of sparsity is the building block of any pruning algorithm. Therefore jaxpruner provides a common API for one-shot\n",
        "pruning. This is achieved by calling the `instant_sparsify` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-3sagbVuOCW"
      },
      "outputs": [],
      "source": [
        "matrix_size = 5\n",
        "learning_rate = 0.01\n",
        "matrix = jax.random.uniform(\n",
        "    jax.random.PRNGKey(8), shape=(matrix_size, matrix_size)\n",
        ")\n",
        "print(matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G51lY-u3xxa6"
      },
      "outputs": [],
      "source": [
        "sparsity_distribution = functools.partial(\n",
        "    jaxpruner.sparsity_distributions.uniform, sparsity=0.8\n",
        ")\n",
        "pruner = jaxpruner.MagnitudePruning(\n",
        "    sparsity_distribution_fn=sparsity_distribution\n",
        ")\n",
        "pruned_matrix, mask = pruner.instant_sparsify(matrix)\n",
        "\n",
        "print(pruned_matrix)\n",
        "print(mask.dtype)\n",
        "print(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMXmrqvACdvA"
      },
      "source": [
        "We can quickly change the sparsity structure using `sparsity_type` flag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCwUUA2SCkTy"
      },
      "outputs": [],
      "source": [
        "pruner = jaxpruner.MagnitudePruning(\n",
        "    sparsity_distribution_fn=sparsity_distribution,\n",
        "    sparsity_type=jaxpruner.sparsity_types.NByM(1, 5),\n",
        ")\n",
        "pruned_matrix, mask = pruner.instant_sparsify(matrix)\n",
        "\n",
        "print(pruned_matrix)\n",
        "print(mask.dtype)\n",
        "print(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W3fHEmMEpk6"
      },
      "source": [
        "`instant sparsify` also supports parameter collections, which are commonly used in deep learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7swztVv1pwN"
      },
      "outputs": [],
      "source": [
        "# params = [matrix, 1 - matrix]\n",
        "params = {'pos': matrix, 'inv': 1 - matrix}\n",
        "pruned_params, masks = pruner.instant_sparsify(params)\n",
        "pprint.pprint(pruned_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E53ARCZdE8a-"
      },
      "source": [
        "It is common to choose different sparsities for different layers or keep them dense entirely. We provide some basic functions to distribute sparsity across different layers such as `uniform` (default) and `erk` under `jaxpruner.sparsity_distributions`. Users can also define their own distributions easily. Here we define a custom distribution function to set different sparsities for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XN2954cfFChC"
      },
      "outputs": [],
      "source": [
        "def custom_distribution(params, sparsity=0.8):\n",
        "  return {key: 0.4 if 'pos' in key else sparsity for key in params}\n",
        "\n",
        "\n",
        "pruner = jaxpruner.MagnitudePruning(\n",
        "    sparsity_distribution_fn=custom_distribution\n",
        ")\n",
        "pruned_params, masks = pruner.instant_sparsify(params)\n",
        "pprint.pprint(jaxpruner.summarize_sparsity(pruned_params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbL4NxLopEY1"
      },
      "source": [
        "Masks used for enforcing sparsity use the same tree structure as the parameters pruned. We use `None` values to indicate dense parameters. We don't create masks for dense variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOaXL58kpZfv"
      },
      "outputs": [],
      "source": [
        "def custom_distribution2(params, sparsity=0.8):\n",
        "  return {key: None if 'pos' in key else sparsity for key in params}\n",
        "\n",
        "\n",
        "pruner = jaxpruner.MagnitudePruning(\n",
        "    sparsity_distribution_fn=custom_distribution2\n",
        ")\n",
        "_, masks = pruner.instant_sparsify(params)\n",
        "pprint.pprint(masks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64jmu55UHzIf"
      },
      "source": [
        "Changing the pruning algorithm is easy as they all inherit from the same `BaseUpdater`. We have the following baseline pruning and sparse training algorithms included in our library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZUIsC1NH5JV"
      },
      "outputs": [],
      "source": [
        "for k in jaxpruner.ALGORITHM_REGISTRY:\n",
        "  print(k, jaxpruner.ALGORITHM_REGISTRY[k])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abfYB4LgH6mB"
      },
      "source": [
        "Next we use gradient based saliency score for pruning. `SaliencyPruning` requires gradients to be passed to `pruner.instant_sparsify`. Gradients are multipled with parameter values to obtain a first order Taylor approximation of the change in loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_1waBgM2Epe"
      },
      "outputs": [],
      "source": [
        "# Gradient based pruning\n",
        "pruner = jaxpruner.SaliencyPruning(\n",
        "    sparsity_distribution_fn=sparsity_distribution\n",
        ")\n",
        "print(pruner.instant_sparsify(matrix, grads=(1 - matrix))[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcdznCj41jat"
      },
      "source": [
        "# Pruning as optimization (jaxpruner + optax)\n",
        "\n",
        "Often state-of-the-art pruning algorithms require iterative adjustments to the sparsity masks used. Such iterative approaches are stateful, i.e. they require some additional variables like masks, counters and initial values. This is similar to common optimization algorithms such as Adam and SGD+Momentum which require moving averages.\n",
        "\n",
        "The observation that *most iterative pruning and sparse training algoritms can be implemented as an optimizer*, played a key role when designing `jaxpruner` and led us to integrate `jaxpruner` with the `optax` optimization library.\n",
        "\n",
        "Here is an example training loop where we find an orthogonal matrix using gradient descent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPVLns38l-sS"
      },
      "outputs": [],
      "source": [
        "matrix_size = 5\n",
        "\n",
        "\n",
        "def loss_fn(params):\n",
        "  matrix = params['w']\n",
        "  loss = jnp.sum((matrix @ matrix.T - jnp.eye(matrix_size)) ** 2)\n",
        "  return loss\n",
        "\n",
        "\n",
        "grad_fn = jax.value_and_grad(loss_fn)\n",
        "\n",
        "\n",
        "@functools.partial(jax.jit, static_argnames='optimizer')\n",
        "def update_fn(params, opt_state, optimizer):\n",
        "  loss, grads = grad_fn(params)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state, loss\n",
        "\n",
        "\n",
        "def run_experiment(init_matrix):\n",
        "  optimizer = optax.sgd(0.05)\n",
        "  params = {'w': init_matrix}\n",
        "  opt_state = optimizer.init(params)\n",
        "\n",
        "  for i in range(20):\n",
        "    params, opt_state, loss = update_fn(params, opt_state, optimizer)\n",
        "    if i % 4 == 0:\n",
        "      print(f'Step: {i}, loss: {loss}')\n",
        "  return params['w']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcOEMg-eUROR"
      },
      "source": [
        "First run the baseline training with a dense matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imi-PawzUS_6"
      },
      "outputs": [],
      "source": [
        "params = jax.random.uniform(\n",
        "    jax.random.PRNGKey(8), shape=(matrix_size, matrix_size)\n",
        ")\n",
        "run_experiment(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMrwGVcOS9Sz"
      },
      "source": [
        "Adding a pruner to an existing training loop requires just 2 lines. First we wrap an existing optimizer using the `pruner.wrap_optax` method. This wrapped optimizer ensures the masks are updated during the training. Second, we add a `pruner.post_gradient_update` call after our gradient step. This function defines algorithm specific parameter updates (like applying a mask to parameters) and provides flexibility when implementing various algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plZ5bqDhT4cR"
      },
      "outputs": [],
      "source": [
        "def run_pruning_experiment(init_matrix, pruner):\n",
        "  optimizer = optax.sgd(0.05)\n",
        "  # Modification #1\n",
        "  optimizer = pruner.wrap_optax(optimizer)\n",
        "\n",
        "  params = {'w': init_matrix}\n",
        "  opt_state = optimizer.init(params)\n",
        "\n",
        "  for i in range(20):\n",
        "    params, opt_state, loss = update_fn(params, opt_state, optimizer)\n",
        "    # Modification #2\n",
        "    params = pruner.post_gradient_update(params, opt_state)\n",
        "\n",
        "    if i % 4 == 0:\n",
        "      print(f'Step: {i}, loss: {loss}')\n",
        "      print(jaxpruner.summarize_sparsity(params, only_total_sparsity=True))\n",
        "  return params['w']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0IifenuM9Ez"
      },
      "source": [
        "Now, prune the matrix in one step (step=15).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqaMojfRNGHd"
      },
      "outputs": [],
      "source": [
        "pruner = jaxpruner.MagnitudePruning(\n",
        "    sparsity_distribution_fn=sparsity_distribution,\n",
        "    scheduler=jaxpruner.sparsity_schedules.OneShotSchedule(target_step=10),\n",
        ")\n",
        "params = jax.random.uniform(\n",
        "    jax.random.PRNGKey(8), shape=(matrix_size, matrix_size)\n",
        ")\n",
        "run_pruning_experiment(params, pruner)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXphssCebrrx"
      },
      "source": [
        "Alternatively we can prune it iteratively using the [polynomial schedule](https://arxiv.org/abs/1710.01878)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg9MAEYsO8YU"
      },
      "outputs": [],
      "source": [
        "pruner = jaxpruner.MagnitudePruning(\n",
        "    sparsity_distribution_fn=sparsity_distribution,\n",
        "    scheduler=jaxpruner.sparsity_schedules.PolynomialSchedule(\n",
        "        update_freq=4, update_start_step=2, update_end_step=14\n",
        "    ),\n",
        ")\n",
        "params = jax.random.uniform(\n",
        "    jax.random.PRNGKey(8), shape=(matrix_size, matrix_size)\n",
        ")\n",
        "run_pruning_experiment(params, pruner)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49mkcCCzTQDQ"
      },
      "source": [
        "# ml_collections.ConfigDict Integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mlb2Vp36b6F-"
      },
      "source": [
        "Many popular jax libraries like [scenic](https://github.com/google-research/scenic) and [big_vision](https://github.com/google-research/big_vision) use `ml_collections.ConfigDict` to configure experiments. `jaxpruner` provides a helper function (`jaxpruner.create_updater_from_config`) to make it easy to use a `ConfigDict` to generate pruner objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKY_GSumHYRI"
      },
      "outputs": [],
      "source": [
        "sparsity_config = ml_collections.ConfigDict()\n",
        "sparsity_config.algorithm = 'magnitude'\n",
        "sparsity_config.update_freq = 2\n",
        "sparsity_config.update_end_step = 15\n",
        "sparsity_config.update_start_step = 5\n",
        "sparsity_config.sparsity = 0.6\n",
        "sparsity_config.dist_type = 'uniform'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uqqg0sYIHWNm"
      },
      "outputs": [],
      "source": [
        "# Create a dense layer and sparsify.\n",
        "pruner = jaxpruner.create_updater_from_config(sparsity_config)\n",
        "params = jax.random.uniform(\n",
        "    jax.random.PRNGKey(8), shape=(matrix_size, matrix_size)\n",
        ")\n",
        "run_pruning_experiment(params, pruner)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16wgrFxCcGXp"
      },
      "source": [
        "# Parallelization with `pmap` and `pjit`\n",
        "\n",
        "The `jaxpruner` library is in general compatible with JAX parallelization mechanisms like `pmap` and `pjit`. There are some minor points to watch out for,\n",
        "which we will now demonstrate using parallelized versions of the previously introduced orthogonal matrix optimization example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDE-QFfCcHSe"
      },
      "source": [
        "## `pmap`\n",
        "\n",
        "First, we demonstrate compatibility with `pmap` where a model is replicated to run different shards of a batch on different devices. Note that this example\n",
        "has no actual model \"inputs\" apart from the parameter matrix and the replication is thus not directly useful, but the general mechanisms are the same as for real training.\n",
        "\n",
        "The main point to watch out for is to make sure that the optimizer state is replicated **after** wrapping it with the `jaxpruner`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8EchfPLEei-"
      },
      "outputs": [],
      "source": [
        "matrix_size = 8\n",
        "\n",
        "\n",
        "def loss_fn(params):\n",
        "  matrix = params['w']\n",
        "  loss = jnp.sum((matrix @ matrix.T - jnp.eye(matrix_size)) ** 2)\n",
        "  return loss\n",
        "\n",
        "\n",
        "grad_fn = jax.value_and_grad(loss_fn)\n",
        "\n",
        "\n",
        "@functools.partial(\n",
        "    jax.pmap,\n",
        "    out_axes=(0, 0, None),\n",
        "    axis_name='batch',\n",
        "    static_broadcasted_argnums=(2,),\n",
        ")\n",
        "def update_fn(params, opt_state, optimizer):\n",
        "  loss, grads = grad_fn(params)\n",
        "  loss = jax.lax.pmean(loss, 'batch')\n",
        "  grads = jax.lax.pmean(grads, 'batch')\n",
        "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state, loss\n",
        "\n",
        "\n",
        "sparsity_distribution = functools.partial(\n",
        "    jaxpruner.sparsity_distributions.uniform, sparsity=0.8\n",
        ")\n",
        "\n",
        "pruner = jaxpruner.MagnitudePruning(\n",
        "    sparsity_distribution_fn=sparsity_distribution,\n",
        "    scheduler=jaxpruner.sparsity_schedules.OneShotSchedule(target_step=0),\n",
        ")\n",
        "\n",
        "optimizer = optax.sgd(0.001)\n",
        "optimizer = pruner.wrap_optax(optimizer)\n",
        "params = {\n",
        "    'w': jax.random.normal(jax.random.PRNGKey(0), (matrix_size, matrix_size))\n",
        "}\n",
        "opt_state = optimizer.init(params)\n",
        "# The key step for using pmap with the jaxpruner is to replicate the optimizer\n",
        "# state **after** wrapping it.\n",
        "opt_state = flax.jax_utils.replicate(opt_state)\n",
        "params = flax.jax_utils.replicate(params)\n",
        "\n",
        "for i in range(100):\n",
        "  params, opt_state, loss = update_fn(params, opt_state, optimizer)\n",
        "  params = pruner.post_gradient_update(params, opt_state)\n",
        "  if i % 5 == 0:\n",
        "    print(f'Step: {i}, loss: {loss}')\n",
        "params = flax.jax_utils.unreplicate(params)\n",
        "print(params['w'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UObDjGBPcTYd"
      },
      "source": [
        "## `pjit`\n",
        "\n",
        "Next, we demonstrate tensor sharded training with `pjit`. Here the key is that the partition specifications of the wrapped optimizer state have to incoporate also the `jaxpruner.base_update.SparseState` produced by the pruning wrapper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp0KIflicNNa"
      },
      "outputs": [],
      "source": [
        "matrix_size = 8\n",
        "if jax.device_count() % 8 == 0:\n",
        "  MESH_SHAPE = (2, 4)\n",
        "else:\n",
        "  MESH_SHAPE = (1, 1)\n",
        "\n",
        "\n",
        "def loss_fn(params):\n",
        "  matrix = params['w']\n",
        "  loss = jnp.sum((matrix @ matrix.T - jnp.eye(matrix_size)) ** 2)\n",
        "  return loss\n",
        "\n",
        "\n",
        "grad_fn = jax.value_and_grad(loss_fn)\n",
        "\n",
        "# Define the partition-specs for pjit; in most libraries for real models this\n",
        "# is done somewhat automatically, yet this will likely require a small\n",
        "# adjustment as shown below.\n",
        "\n",
        "params_partition = {'w': PartitionSpec('X', 'Y')}\n",
        "\n",
        "# The main step required to run the jaxpruner together with pjit is defining\n",
        "# a partition-spec for the wrapped `SparseState` as shown below.\n",
        "opt_partition = jaxpruner.base_updater.SparseState(\n",
        "    masks=params_partition,\n",
        "    inner_state=(None, None),  # other optimizers may require sharding\n",
        "    target_sparsities=None,\n",
        "    count=None,\n",
        ")\n",
        "\n",
        "resources = (params_partition, opt_partition)\n",
        "\n",
        "\n",
        "@functools.partial(\n",
        "    jax.experimental.pjit.pjit,\n",
        "    in_shardings=resources,\n",
        "    out_shardings=resources + (None,),\n",
        "    static_argnames='optimizer',\n",
        ")\n",
        "def update_fn(params, opt_state, optimizer):\n",
        "  loss, grads = grad_fn(params)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state, loss\n",
        "\n",
        "\n",
        "sparsity_distribution = functools.partial(\n",
        "    jaxpruner.sparsity_distributions.uniform, sparsity=0.8\n",
        ")\n",
        "pruner = jaxpruner.MagnitudePruning(\n",
        "    sparsity_distribution_fn=sparsity_distribution,\n",
        "    scheduler=jaxpruner.sparsity_schedules.OneShotSchedule(target_step=0),\n",
        ")\n",
        "\n",
        "optimizer = optax.sgd(0.001)\n",
        "optimizer = pruner.wrap_optax(optimizer)\n",
        "params = {\n",
        "    'w': jax.random.normal(jax.random.PRNGKey(0), (matrix_size, matrix_size))\n",
        "}\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "devices = np.asarray(jax.devices()).reshape(MESH_SHAPE)\n",
        "mesh = jax.sharding.Mesh(devices, ('X', 'Y'))\n",
        "\n",
        "with mesh:\n",
        "  for i in range(100):\n",
        "    params, opt_state, loss = update_fn(params, opt_state, optimizer)\n",
        "    params = pruner.post_gradient_update(params, opt_state)\n",
        "    if i % 5 == 0:\n",
        "      print(f'Step: {i}, loss: {loss}')\n",
        "  print(params['w'])\n",
        "  jax.debug.visualize_array_sharding(params['w'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWKbfqVQD-tJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "quick_start.ipynb",
      "private_outputs": true,
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

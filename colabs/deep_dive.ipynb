{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wqeWSSKm7EjK"
      },
      "outputs": [],
      "source": [
        "#@title LICENSE\n",
        "# Licensed under the Apache License, Version 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFEU55-OTDiT"
      },
      "source": [
        "# JaxPruner Deep Dive\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-research/jaxpruner/blob/main/colabs/deep_dive.ipynb)\n",
        "\n",
        "In this interactive colab we make a deep dive on the internals of the `jaxpruner` library and demonstrate how to implement a new algorithm. We will \n",
        "start w/ going over our base class `BaseUpdater` and sub-class it to implement our new algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdzHCJuop4ZB"
      },
      "outputs": [],
      "source": [
        "\n",
        "import dataclasses\n",
        "import chex\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lZYvAFg7RoH"
      },
      "outputs": [],
      "source": [
        "!pip3 install git+https://github.com/google-research/jaxpruner\n",
        "import jaxpruner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaU4vsg-tfrq"
      },
      "source": [
        "## The BaseUpdater\n",
        "We define most of the common functionalities used in pruning/sparse-training algorithms under the BaseUpdater class. All algorithms are expected to inherit this class and implement at least the `calculate_scores` function. BaseUpdater provide 2 main entry-points for the user:\n",
        "\n",
        "- `instant_sparsify`: Used for instant (one-shot) pruning.\n",
        "- `wrap_optax`: Used to wrap an existing optimizer with pruning related operations. \n",
        "\n",
        "Using `instant_sparsify` is straight-forward and demonstrated in our [Quick-Start colab](TODO). In this colab we will focus on training/optimization based algorithms which use the `wrap_optax` functionality.\n",
        "\n",
        "Optax optimizers are [gradient transformations](https://optax.readthedocs.io/en/latest/api.html#optax.GradientTransformation) that update provided state variables to control how gradients are used. Resulting gradients are often added to the paramaters in a separate call outside the optimizer. This means every pruning algorithm is able to access gradients directly and update the gradients together with its own state (masks, counter, etc.).\n",
        "\n",
        "Below we share a simplified version of `wrap_optax` function. `self.init_state` is called during the initialization and similarly `self.update_state` is used if `self.scheduler` says it is an update iteration. The state of the original optax transformation is stored under `.inner_state`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPNafKsrOi3l"
      },
      "outputs": [],
      "source": [
        "def wrap_optax(self, inner: optax.GradientTransformation\n",
        "               ) -\u003e optax.GradientTransformation:\n",
        "  \"\"\"Wraps an existing transformation and adds sparsity related updates.\"\"\"\n",
        "  def init_fn(params):\n",
        "    sparse_state = self.init_state(params)\n",
        "    sparse_state = sparse_state._replace(inner_state=inner.init(params))\n",
        "    return sparse_state\n",
        "\n",
        "  def update_fn(updates, state, params):\n",
        "    # Simplified\n",
        "    if self.scheduler.is_mask_update_iter(state.count):\n",
        "      new_state = self.update_state(state, params, updates)\n",
        "    else:\n",
        "      new_state = state\n",
        "    new_updates, updated_inner_state = inner.update(updates, new_state.inner_state, params)\n",
        "    new_state = new_state._replace(inner_state=updated_inner_state,\n",
        "                                   count=new_state.count + 1)\n",
        "    return new_updates, new_state\n",
        "\n",
        "  return optax.GradientTransformation(init_fn, update_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g_DEMZ4Ty0h"
      },
      "source": [
        "### `init_state` and `update_state` \n",
        "There are 2 main functions called during the optimizer initialization and usage. These are used to implement pruning and sparse_training related operations. Let's start with `init_state`. `jaxpruner` implements sparsity through binary masks. We create these masks at initialization and store it using the `SparseState`. The smallest data format in jax is `int8` (boolean variables use 8 bits per element). We use `jnp.packbits` to compress the masks further. In addition to `masks` we also store a count variable and target_sparsities under the `SparseState`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rYkrfy_Twb7"
      },
      "outputs": [],
      "source": [
        "def init_state(self, params: chex.ArrayTree):\n",
        "  \"\"\"Creates the sparse state.\"\"\"\n",
        "  if self.sparsity_distribution_fn is None:\n",
        "    target_sparsities = None\n",
        "  else:\n",
        "    target_sparsities = self.sparsity_distribution_fn(params)\n",
        "  logging.info('target_sparsities: %s', target_sparsities)\n",
        "  masks = self.get_initial_masks(params, target_sparsities)\n",
        "  if self.use_packed_masks:\n",
        "    masks = jax.tree.map(jnp.packbits, masks)\n",
        "  return SparseState(\n",
        "      masks=masks,\n",
        "      target_sparsities=target_sparsities,\n",
        "      count=jnp.zeros([], jnp.int32),\n",
        "  )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWgV4uPrYbMx"
      },
      "source": [
        "Different algorithms have different routines for updating masks. In `BaseUpdater` we implement a routine best suited for gradual pruning algorithms. This routine calculates sparsities using the current step count and \n",
        "creates masks using the scores. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g88YH--0Txqz"
      },
      "outputs": [],
      "source": [
        "def update_state(self, sparse_state: jaxpruner.SparseState, params: chex.ArrayTree,\n",
        "                 grads: chex.ArrayTree) -\u003e jaxpruner.SparseState:\n",
        "  \"\"\"Updates the sparse state.\"\"\"\n",
        "  sparsities = self.scheduler.get_sparsity_at_step(\n",
        "      sparse_state.target_sparsities, sparse_state.count\n",
        "  )\n",
        "  scores = self.calculate_scores(\n",
        "      params, sparse_state=sparse_state, grads=grads\n",
        "  )\n",
        "  new_masks = self.create_masks(scores, sparsities)\n",
        "  if self.use_packed_masks:\n",
        "    new_masks = jax.tree.map(jnp.packbits, new_masks)\n",
        "  return sparse_state._replace(masks=new_masks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSG2FaWuZONV"
      },
      "source": [
        "### Other helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ3X61inkbvS"
      },
      "source": [
        "`self.update_state` function above has access to parameters, but can't update them directly. However, most pruning and sparse training algorithms require some updates like applying masks to parameters. To address this limitation in a unified manner, we provide two additional functions. These functions are expected to be added to the training loop by the user. According to the algorithm chosen; they do necessary updated on the parameters. These functions are:\n",
        "- `post_gradient_update`: Intended to be called after applying gradients to the parameters. Since most algorithms keep parameters *sparse*, the default implementation under `BaseUpdater` applies masks to the parameters provided.\n",
        "```python\n",
        "def post_gradient_update(\n",
        "      self, params: chex.ArrayTree, sparse_state: SparseState\n",
        "  ) -\u003e chex.ArrayTree:\n",
        "    return self.apply_masks(params, sparse_state.masks)\n",
        "```\n",
        "- `pre_forward_update`: Intended to be called before the forward call (i.e. `flax_model.apply(data, params)`) to modify the parameters temporarily. This is useful when implementing algorithms like [STE](https://arxiv.org/abs/2102.04010) which calls top-k operation on parameters before every forward call, however keeps the original parameters as it is. Most algorithms doesn't need this call to work and therefore the default implementation is an *identity* function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkX_N6S5n9t6"
      },
      "source": [
        "### Configuration\n",
        "`BaseUpdater` has following attributes:\n",
        "\n",
        "- `scheduler`:  Implements when masks are updated and how much sparsity should be applied at a given step. Default is `NoUpdateSchedule`.\n",
        "- `skip_gradients`:  Returns zero gradients during mask update iterations, practically skipping the gradient update (default=False).\n",
        "- `is_sparse_gradients`: Whether masks are applied to the gradients before passing to the optimizer wrapped (default=False).\n",
        "- `sparsity_type`: One of `sparsity_types.{Unstructured/NbyM/Block}`. Determines the topk_function used by algorithms (default=Unstructured).\n",
        "- `sparsity_distribution_fn(params, ...)`: Function to set target sparsity for each parameter. Default value is `uniform` distribution with `None` sparsity.\n",
        "- `rng_seed`: random seed to overwrite the default (default=8).\n",
        "- `use_packed_masks` If true, packs int8 masks into bits (default=False). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_gtojKbZSN0"
      },
      "source": [
        "## Implementing new algorithms\n",
        "Since `BaseUpdater` implements most of the routines needed by the gradual pruning, therefore implementing gradual magnitude pruning algorithm requires only an implementation of the `calculate_scores` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NI6_-Y49ZUNH"
      },
      "outputs": [],
      "source": [
        "@dataclasses.dataclass\n",
        "class MagnitudePruning(jaxpruner.BaseUpdater):\n",
        "  \"\"\"Implements magnitude based pruning.\"\"\"\n",
        "\n",
        "  def calculate_scores(self, params, sparse_state=None, grads=None):\n",
        "    del sparse_state, grads\n",
        "    param_magnitudes = jax.tree.map(jnp.abs, params)\n",
        "    return param_magnitudes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9mvgAFSoLCy"
      },
      "source": [
        "`calculate_scores` is used by `update_state` and `instant_sparsify` function. An alternative way to sub-class `BaseUpdater` is through updating these higher level functions. Below we implement a static sparse training method that sparsifies the masks randomly at\n",
        "initialization and keeps them same. \n",
        "\n",
        "We can also over-write fields defined in BaseUpdater dataclass. When doing that, **we need to make sure new or overwritten \n",
        "variables are defined using `dataclasses.field` or with type annotation**. If not,\n",
        "these variables are treated as class variables and overwritten at initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VL6fbUL1oy19"
      },
      "outputs": [],
      "source": [
        "@dataclasses.dataclass\n",
        "class StaticRandomSparse(jaxpruner.BaseUpdater):\n",
        "  \"\"\"Initializes sparsity randomly and optimizes using that sparsity.\"\"\"\n",
        "\n",
        "  is_sparse_gradients: bool = True\n",
        "\n",
        "  def update_state(self, sparse_state, params, grads):\n",
        "    \"\"\"Returns sparse_state unmodified.\"\"\"\n",
        "    del params, grads\n",
        "    return sparse_state\n",
        "\n",
        "  def get_initial_masks(\n",
        "      self, params: chex.ArrayTree, target_sparsities: chex.ArrayTree\n",
        "  ) -\u003e chex.ArrayTree:\n",
        "    \"\"\"Generate initial mask. This is only used when .wrap_optax() is called.\"\"\"\n",
        "    scores = pruners.generate_random_scores(params, self.rng_seed)\n",
        "    init_masks = self.create_masks(scores, target_sparsities)\n",
        "    return init_masks\n",
        "\n",
        "  def instant_sparsify(self, params, grads=None):\n",
        "    raise RuntimeError(\n",
        "        'instant_sparsify function is not supported in sparse training methods.'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B6moS4-pmSH"
      },
      "source": [
        "There are often multiple paths to implementing same algorithm. Feel free to check other algorithms to get inspired further."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "deep_dive.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
